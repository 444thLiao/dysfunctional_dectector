"""
strategy brief:

1. convert CDS and IG into a table with position information (genomic position)
2. blast search pseudogenized ones against a smaller nr database
    a. for each row of CDS/IG, determine is it a pseudogene or not
        i. if not, try to join prev pseudogenes or just leave them
        ii. if yes, go to step b
    b. keep determineing whether the collected pseudogenes are matched to a same gene and continuous locations
        ia. if there are two pseudogenes and they are jointable(see note1), leave them since it might have the third pieces.
        ib. if not, drop the first one and continue. (not drop them all since the last one might have the remaining pieces)

        ii. if there are n (n>=3) pseudogenes and all of them are not continuous, join (n-1) pseudogenes.
a tries to find those pseudogenes are continous on the original genome
b tries to find those pseudogenes are continous on the matched genes

note1: jointable means the two pseudogenes are continous on the matched genes. they should follow the following criteria:
    strain order are the same.
    after merge, they should cover at least 50% of the matched gene
    two neighboring pseudogenes not too far (default:30bp)

version history
20230510: correct the error that join two distant pseudogenes. But this kind of pseudogenes is likely splitted by an insertion (noted).
20230606: make it a binary script.
"""

from glob import glob
import pandas as pd
from tqdm import tqdm
from Bio import SeqIO
from os.path import exists,dirname,realpath,join,isdir
import os,click
from collections import defaultdict
import warnings
warnings.filterwarnings("ignore")
from dysfunctional_dectector.src.utilities.tk import check,parse_gbk
from dysfunctional_dectector.src.utilities.logging import logger

debug = False
################################# parameters
default_nrfna = '/home-db/pub/protein_db/nr/v20230523/nr'

#################################


#############
def get_gfffile(inpattern):
    if inpattern is None:
        return []
    allf = []
    for e in inpattern.split(','):
        e = e.strip()
        f = [realpath(_) for _ in glob(e)]
        allf.extend(f)
    logger.debug(f"found {len(allf)} gff file.")
    return allf

@click.group()
@click.option('--inpattern','-i',help="Path of gff generated by pseudofinder. It should contain * for showing all gff. It could be separeted by comma. For example, './*.gff,../*.gff'  " )
@click.option('--odir','-o',help="output directory")
@click.option('--suffix','-s',help="suffix for extract genome ID. default is '_nr'  ",default='_nr')
@click.option('--sub_nr','-snr',help="subset of nr file.")
@click.option('--nr_path','-nr',help=f"path of nr fasta file. default in {default_nrfna}",
              default=default_nrfna)
@click.pass_context
def cli(ctx,inpattern,odir,suffix,sub_nr,nr_path):
    ctx.ensure_object(dict)
    gff_list = get_gfffile(inpattern)
    ctx.obj['inpattern'] = gff_list
    ctx.obj['suffix'] = suffix
    gnames = []
    for f in gff_list:
        gname = f.split('/')[-1].split(suffix)[0]
        gnames.append(gname)
    ctx.obj['names'] = gnames
    ctx.obj['nr_path'] = nr_path
    ctx.obj['odir'] = odir
    if sub_nr is not None:
        ctx.obj['subnr'] = sub_nr
    else:
        ctx.obj['subnr'] = f'{odir}/subnr'
    
@cli.command()
@click.pass_context
def step1(ctx,):
    gff_list = ctx.obj['inpattern']
    suffix = ctx.obj['suffix']
    odir = ctx.obj['odir']
    names = ctx.obj['names']
    nr_fna = ctx.obj['nr_path']
    ########
    all_pids = set()
    for f,gname in tqdm(zip(gff_list,names),total=len(gff_list),
                        desc='# of GFF files for subj.pids: '):
        if 'merged_for' in f:
            continue
        f1 = f'{dirname(f)}/{gname}{suffix}_proteome.faa.blastP_output.tsv'
        f2 = f'{dirname(f)}/{gname}{suffix}_intergenic.fasta.blastX_output.tsv'
        df = pd.read_csv(f1,sep='\t',header=None)
        df2 = pd.read_csv(f2,sep='\t',header=None)
        mdf = pd.concat([df,df2],axis=0)
        pids = mdf.sort_values(10).groupby(0).head(1)
        pseudo_ = pd.read_csv(f,comment='#',sep='\t',header=None)
        pseudo_.loc[:,'locus'] = [_.split('=')[-1] for _ in pseudo_[8]]
        locus_ = [_ for v in pseudo_['locus'] for _ in v.split(',')]
        all_pids = all_pids.union(pids.loc[pids[0].isin(locus_),1])
    ## time consuming step
    with open(f'{odir}/subj.pids','w') as f1:
        f1.write('\n'.join(all_pids))
    subnr_db = ctx.obj['subnr']
    if exists(subnr_db):
        seqs = [_.id for _ in SeqIO.parse(subnr_db,'fasta')]
        missing_pids = set(all_pids) - set(seqs)
        logger.debug(f"Existed subnr_db file, but we need to add {len(missing_pids)} more to it ")
        with open(f'{odir}/subj_extra.pids','w') as f1:
            f1.write('\n'.join(missing_pids))
        if missing_pids:
            os.system(f"seqtk subseq {nr_fna} {odir}/subj_extra.pids >> {subnr_db}")
        os.system(f"diamond makedb --in {subnr_db} -d {subnr_db}.dmnd")
    else:
        logger.debug(f"Run time consumeing step. Be patient (extracting sequences from nr file )")
        os.system(f"seqtk subseq {nr_fna} {odir}/subj.pids > {subnr_db}")
        os.system(f"diamond makedb --in {subnr_db} -d {subnr_db}.dmnd")
    

def parse_posdf(gbk,intergenic_fasta,pseudo_fasta):
    records = parse_gbk(gbk)
    #intergenic_fasta = gid2ig[gname]
    ign2pos = {}
    for r in SeqIO.parse(intergenic_fasta,'fasta'):
        name = r.description
        ignname,contig,pos = name.split(' ')
        pos = pos.split(']')[0].strip('[').split(':')
        pos = [int(_) for _ in pos]
        ign2pos[ignname] = [contig] + pos
    pos_df = pd.DataFrame(records).T
    d = pd.DataFrame(ign2pos).T
    pos_df = pd.concat([pos_df,d],axis=0)
    pos_df = pos_df.sort_values([0,1])
    pseudo_ids = [r.id for r in SeqIO.parse(pseudo_fasta,'fasta')]
    pos_df.loc[pseudo_ids,'pseudo'] = 'Yes'
    pos_df = pos_df.sort_values([0,1])
    #pos_df.to_csv(f'{odir}/{gname}_pos.tsv',sep='\t')
    return pos_df

    
@cli.command()
@click.option('--gbkpattern','-gp',help="genbank pattern for all gbk files")
@click.pass_context
def step2(ctx,gbkpattern):
    #### write out the pseudo gene sequences (nucl)
    tqdm.write(f"write out *_pos.tsv and Blast search with a smaller db. ")
    gff_list = ctx.obj['inpattern']
    suffix = ctx.obj['suffix']
    odir = ctx.obj['odir']
    names = ctx.obj['names']
    subnr_db = ctx.obj['subnr']
    ########
    nodir = f"{odir}/merged_for/"
    if not exists(nodir):
        os.makedirs(nodir)
    for gff,gname in tqdm(zip(gff_list,names),desc='# of GFF files for *pseudos.fasta: '):
        if 'merged_for/' in gff:
            continue
        if exists(f'{nodir}/{gname}_pseudos.fasta'):
            continue
        records = []
        f1 = gff.replace('_pseudos.gff','_cds.fasta')
        records += list(SeqIO.parse(f1,'fasta'))
        f1 = gff.replace('_pseudos.gff','_intergenic.fasta')
        records += list(SeqIO.parse(f1,'fasta'))
        pseudo_ = pd.read_csv(gff,comment='#',sep='\t',header=None)
        pseudo_.loc[:,'locus'] = [_.split('=')[-1] for _ in pseudo_[8]]
        locus_ = [_ for v in pseudo_['locus'] for _ in v.split(',')]
        with open(f'{nodir}/{gname}_pseudos.fasta','w') as f:
            SeqIO.write([_ for _ in records if _.id in locus_],f,'fasta')
    #### blastx search using pseudo-gene sequences against smaller nr database
    for fna in tqdm(glob(f"{nodir}/*_pseudos.fasta"),desc='# of pseudo-fasta files: '):
        gname = fna.split('/')[-1].split('_')[0]
        cmd = f"diamond blastx --db {subnr_db}.dmnd -q {fna} -f 6 > {nodir}/{gname}{suffix}_pseudos.fasta.blastX_output.tsv -k 1000000"
        if not exists(f"{nodir}/{gname}{suffix}_pseudos.fasta.blastX_output.tsv") or os.path.getsize(f"{nodir}/{gname}{suffix}_pseudos.fasta.blastX_output.tsv")==0:
            os.system(cmd)
            
    allgbks = []
    for e in gbkpattern.split(','):
        e = e.strip()
        f = [realpath(_) for _ in glob(e)]
        allgbks.extend(f)
    allgbks = [gbk for gbk in allgbks 
               if gbk.split('/')[-1].replace('.gbk','') in names]    
    logger.debug(f"found {len(allgbks)} for {len(names)} genomes.")
    name2gbk = {gbk.split('/')[-1].replace('.gbk',''):gbk for gbk in allgbks}
    for gff,gname in tqdm(zip(gff_list,names),desc='# of GFF files for *_pos.tsv: '):
        if exists(f'{nodir}/{gname}_pos.tsv'):
            continue
        gbk = name2gbk[gname]
        IG_fna = gff.replace('_pseudos.gff','_intergenic.fasta')
        pseudo_fna = f'{nodir}/{gname}_pseudos.fasta'
        if not exists(f'{nodir}/{gname}_pos.tsv'):
            pos_df = parse_posdf(gbk,IG_fna,pseudo_fna)
            pos_df.columns = [int(_) if _!='pseudo' else _ for _ in pos_df.columns]
            pos_df.to_csv(f'{nodir}/{gname}_pos.tsv',sep='\t')
        # else:
        #     pos_df = pd.read_csv(f'{nodir}/{gname}_pos.tsv',sep='\t',index_col=0)
        #     pos_df.columns = [int(_) if _!='pseudo' else _ for _ in pos_df.columns]


def determine_continuous(refbased_loc,length,min_dis=30,verbose=False):
    cov = [pos for s,e in refbased_loc for pos in range(s,e+1)]
    cov_ratio = len(set(cov))/length
    if len(set([e>s for s,e in refbased_loc]))!=1:
        # not at the same direction
        if verbose: print('random orders')
        return False
    iter = list(zip(refbased_loc[:-1],refbased_loc[1:]))
    i1,i2 = iter[0]
    if i2[0]-i1[1]<=0:
        c = refbased_loc[::-1]
        iter = list(zip(c[:-1],c[1:]))
    for i1,i2 in iter:
        if i2[0]-i1[1]>=min_dis:
            if verbose: print(f'too far: {i2[0]-i1[1]}')
            return False
    if cov_ratio>0.5:
        if verbose: print('merged still too short')
        return True
    elif cov_ratio >1.2:
        if verbose: print('cover ratio too large')
        return False
    if verbose: print('final',cov_ratio)
    return False

def get_refbased_loc(hits,shared_hits,locus2length):
    shared_ref = list(shared_hits)[0]
    refbased_loc = []
    for (prev_id,sdf) in hits:
        q_full_length = locus2length[prev_id]
        _ssdf = sdf.loc[sdf['sseqid']==shared_ref,['qstart','qend','sstart','send']]
        if _ssdf.shape[0]==1:
            qs,qe, s,e = _ssdf.iloc[0].values
            qs,qe = min([qs,qe]),max([qs,qe])
            qlength = qe-qs+1
            if debug:
                print(prev_id,qlength,q_full_length,s,e)
            if qlength < q_full_length:
                if s<e:
                    s-= qs-1
                    e+= q_full_length-qe
                else:
                    s+= q_full_length-qe
                    e-= qs-1
            if debug:        
                print('A',qlength,q_full_length,s,e)
        else:
            all_pos = list(_ssdf.values[:,-2:])
            if len(set([e>s for s,e in all_pos]))==1:
                all_s = [_ for v in all_pos for _ in v]
                s,e = min(all_s),max(all_s)
                all_qs = [_ for v in list(_ssdf.iloc[:,:2].values) for _ in v]
                qs,qe = min(all_qs),max(all_qs)
                qlength = qe-qs+1
                if debug:
                    print(prev_id,qlength,q_full_length,s,e)
                if qlength < q_full_length:
                    s-= qs-1
                    e+= q_full_length-qe  
                    if debug:        
                        print('A',qlength,q_full_length,s,e)          
            else:
                s,e = all_pos[0]
                qs,qe = list(_ssdf.iloc[0,:2])
                qlength = qe-qs+1
                if qlength < q_full_length:
                    s-= qs-1
                    e+= q_full_length-qe                  
        refbased_loc.append((s,e))
    return shared_ref,refbased_loc

@cli.command()
@click.option('--uni','-u',help="input directory of some files.",default=None,required=False)
@click.pass_context
def step3(ctx,uni=None):
    """
    read subnr_db, pseudogenes.fasta
    """
    #### main
    tqdm.write(f"merge neighboring pseudogenes into one row since they can be merged")
    gff_list = ctx.obj['inpattern']
    suffix = ctx.obj['suffix']
    odir = ctx.obj['odir']
    names = ctx.obj['names']
    subnr_db = ctx.obj['subnr']
    if uni is None:
        nodir = f"{odir}/merged_for/"
    else:
        nodir = uni
    ########
    ref2len = {}
    for r in SeqIO.parse(subnr_db,'fasta'):
        ref2len[r.id] = len(r.seq)
    if len(names)==1:
        iter_n = names
    else:
        iter_n = tqdm(names,desc='# of genomes')
    gname2merged_pseudo = {}
    for gname in iter_n:
        subffn = f"{nodir}/{gname}_pseudos.fasta"
        sub2len = {}
        for r in SeqIO.parse(subffn,'fasta'):
            sub2len[r.id] = len(r.seq)
        posfile = f'{nodir}/{gname}_pos.tsv'
        pos_df = pd.read_csv(posfile,sep='\t',index_col=0)
        pos_df.columns = [int(_) if _!='pseudo' else _ 
                          for _ in pos_df.columns]
        
        blastxfile =  f'{nodir}/{gname}{suffix}_pseudos.fasta.blastX_output.tsv'
        new_blastdf = pd.read_csv(blastxfile,sep='\t',header=None)
        new_blastdf.columns = ['qseqid','sseqid','pident','length','mismatch',
                               'gapopen','qstart','qend','sstart','send','evalue','bitscore']
        merged_pseudos = []
        prev_hits = []
        for region_id,pos in pos_df.iterrows():
            # current one is a functional gene
            if str(pos['pseudo'])=='nan' and len(prev_hits)>1:
                # functional gene next to pseudogenes (should start merging?)
                if debug:
                    print('merged directly',region_id,)
                    break
                shared_hits = set.intersection(*[set(sdf['sseqid']) for (prev_id,sdf) in prev_hits])
                shared_ref, refbased_loc = get_refbased_loc(prev_hits,shared_hits,sub2len)
                if determine_continuous(refbased_loc,ref2len[shared_ref]):
                    merged_pseudos.append([prev_id for (prev_id,sdf) in prev_hits] + [shared_ref])
                # start a new round.
                prev_hits = []
                continue
            elif str(pos['pseudo'])=='nan' and len(prev_hits)<=1:
                # functional genes or only one pseudogene (skip it)
                prev_hits = []
                continue
            # current one is a pseudogene
            hits = new_blastdf.loc[new_blastdf['qseqid']==region_id,:]
            prev_hits.append([region_id,hits])
            if len(prev_hits)==1:
                # only add one, no need to run the following codes
                continue
            ### elongating the prev_hits (with at least 2 hits)
            shared_hits = set.intersection(*[set(sdf['sseqid']) for (prev_id,sdf) in prev_hits])
            related_contigs = list(set(pos_df.loc[[_[0] for _ in prev_hits],0]))

            ## for detecting should stop elongating
            if (len(shared_hits)==0 and len(prev_hits)>2) or (len(related_contigs)>1 and len(prev_hits)>2):
                ## all (>=3) of them all not continuous (hit-based positions from the blastx). but all-1 are related. So start to merge
                ## OR: found genes are located at two contigs
                if debug:
                    print('Found critera1',region_id,)
                shared_hits = set.intersection(*[set(sdf['sseqid']) for (prev_id,sdf) in prev_hits[:-1]])
                shared_ref, refbased_loc = get_refbased_loc(prev_hits[:-1],shared_hits,sub2len)
                if determine_continuous(refbased_loc,ref2len[shared_ref]):
                    merged_pseudos.append([prev_id for (prev_id,sdf) in prev_hits[:-1]] + [shared_ref])
                # start a new round but keep the last one since we didn't process the last one
                prev_hits = prev_hits[-1:]
            elif (len(shared_hits)==0 and len(prev_hits)==2) or (len(related_contigs)>1 and len(prev_hits)==2):
                if debug:
                    print('Found critera2',region_id,)                
                # 1. previous two are not continuous, no need to elongate, end it.
                # OR: 2. new one and the old one are from two contigs.
                # star a new round but keep the last one
                prev_hits = prev_hits[-1:]
                continue
        gname2merged_pseudo[gname] = merged_pseudos
    text = ''
    for genome,merged_pseudos in gname2merged_pseudo.items():
        text += '\n'.join([genome + '\t' + '\t'.join(_[:-1]) for _ in merged_pseudos])+'\n'
    with open(f"{odir}/merged_pseudos.txt",'w') as f:
        f.write(text)
    

@cli.command()
@click.pass_context
def merge(ctx,):
    tqdm.write(f"Use step3 generated file to merge pseudogenes and generate a summary file.")
    #### main
    gff_list = ctx.obj['inpattern']
    suffix = ctx.obj['suffix']
    odir = ctx.obj['odir']
    names = ctx.obj['names']
    subnr_db = ctx.obj['subnr']
    nodir = f"{odir}/merged_for/"
    ########
    if len(names)==1:
        iter_n = zip(gff_list,names)
    else:
        iter_n = tqdm(zip(gff_list,names))
    dfs = []
    for gff,gname in iter_n:
        _df = pd.read_csv(gff,comment='#',sep='\t',header=None)
        _df.loc[:,'genome'] = gname
        dfs.append(_df)
    pseudo_df_nano = pd.concat(dfs,axis=0)
    pseudo_df_nano.columns = ['contig', 'soft', 'gene', 'start', 'end', 't', 'strand', 'e', 'note','genome']
    pseudo_df_nano.loc[:,'length'] = (pseudo_df_nano['end'] - pseudo_df_nano['start']).abs()
    pseudo_df_nano.index = [f"{row['contig']}:{row['start']}-{row['end']}" for i,row in pseudo_df_nano.iterrows()]
    pseudo_df_nano.loc[:,'pseudo name'] = [_.split(';')[-2].split('=')[-1] for _ in tqdm(pseudo_df_nano['note'])]
    #pseudo_df_nano.loc[:,'genome'] = [_.split('_')[0] for _ in tqdm(pseudo_df_nano['contig'])]
    pseudo_df_nano.to_csv(f"{odir}/pseudofinder_Unmerged.tsv",sep='\t',index=1)
    
    gname2merged_pseudo = defaultdict(list)
    for r in open(f"{odir}/merged_pseudos.txt").read().strip().split('\n'):
        rows = r.split('\t')
        gname2merged_pseudo[rows[0]].append(tuple(rows[1:]))

    final_merged_dfs = []
    for genome,merged_pseudo in tqdm(gname2merged_pseudo.items(),
                                     desc='Number of merged pseudos: '):
        sub_df = pseudo_df_nano.loc[pseudo_df_nano['genome']==genome,:]
        
        ordered_l = []
        ori_l2pseudo_l = {}
        for i,row in sub_df.iterrows():
            n = row['note'].split(';')
            pseudo_l = n[-2].split('=')[-1]
            ori_l = n[-1].split('=')[-1]
            for l in ori_l.split(','):
                ori_l2pseudo_l[l]=pseudo_l
            ordered_l.append(pseudo_l)
        new_pseudo = sub_df.copy()
        needed_merged = []
        for p in merged_pseudo:
            p = [ori_l2pseudo_l[_] for _ in p if _ in ori_l2pseudo_l]
            all_i = new_pseudo.loc[new_pseudo['pseudo name'].isin(p),:].index
            needed_merged.append(all_i)
        all_merged_idx = list(set([_ for v in needed_merged for _ in v if len(_)!=1]))
        remained_pseudo = new_pseudo.loc[new_pseudo.index.difference(all_merged_idx),:]
        sub_pseudo = new_pseudo.loc[all_merged_idx,:]

        merged_dfs = []
        for all_i in needed_merged:
            if len(all_i)<=1:continue
            sub_df = sub_pseudo.loc[all_i,:]
            s = int(sub_df[['start','end']].min().min())
            e = int(sub_df[['start','end']].max().max())
            idx = f"{sub_df.iloc[0,0]}:{s}-{e}"
            if idx in sub_pseudo.index:
                continue
            new_one = sub_df.iloc[[0],:]
            new_one.index = [idx]
            new_one.loc[idx,['start','end','length','note','merged']] = s,e,e-s+1, '.'.join(sub_df['note']), ';'.join(all_i)
            merged_dfs.append(new_one)
        # sub_pseudo.loc[idx,['start','end','length','note','merged']] = s,e,e-s+1, '.'.join(sub_df['note']), ';'.join(all_i)
        # sub_pseudo = sub_pseudo.drop(all_i)
        final_merged_pseudo = pd.concat([remained_pseudo]+merged_dfs,axis=0)
        final_merged_dfs.append(final_merged_pseudo)
    final_merged_pseudo = pd.concat(final_merged_dfs,axis=0)
    #assert final_merged_pseudo.shape[0] == len(set(final_merged_pseudo.index))
    final_merged_pseudo.to_csv(f"{odir}/pseudofinderALL_w_annot_MERGEDcontinuous.tsv",sep='\t',index=1)
    
    
if __name__ == '__main__':
    cli()
    # cd /mnt/ivy/thliao/project/coral_ruegeria/nanopore_processing/annotations/pseudofinder ;  python /mnt/ivy/thliao/project/coral_ruegeria/merged_pseudogenes_bin.py -i './*/*_nr_pseudos.gff,/mnt/ivy/thliao/project/coral_ruegeria/data_processing/pub_dataset/pseudogenes/*/*_nr_pseudos.gff' -o ./ step1; 
    # python /mnt/ivy/thliao/project/coral_ruegeria/merged_pseudogenes_bin.py -i './*/*_nr_pseudos.gff,/mnt/ivy/thliao/project/coral_ruegeria/data_processing/pub_dataset/pseudogenes/*/*_nr_pseudos.gff' -o ./ step2 -gp '/mnt/ivy/thliao/project/coral_ruegeria/nanopore_processing/canu_o/*/09_prokka/*.gbk,/mnt/ivy/thliao/project/coral_ruegeria/data_processing/pub_dataset/prokka_o/*/*.gbk' ; 
    # python /mnt/ivy/thliao/project/coral_ruegeria/merged_pseudogenes_bin.py -i './*/*_nr_pseudos.gff,/mnt/ivy/thliao/project/coral_ruegeria/data_processing/pub_dataset/pseudogenes/*/*_nr_pseudos.gff' -o ./ step3; 
    # python /mnt/ivy/thliao/project/coral_ruegeria/merged_pseudogenes_bin.py -i './*/*_nr_pseudos.gff,/mnt/ivy/thliao/project/coral_ruegeria/data_processing/pub_dataset/pseudogenes/*/*_nr_pseudos.gff' -o ./ merge



"""
python /home-user/thliao/script/dysfunctional_dectector/src/pseudofinder_api/merged_pseudo.py -i /mnt/ivy/thliao/project/coral_ruegeria/nanopore_processing/annotations/DFD/s1out/pseudofinder/*/*_nr_pseudos.gff -o /mnt/ivy/thliao/project/coral_ruegeria/nanopore_processing/annotations/DFD/s1out/pseudofinder step1 ; python /home-user/thliao/script/dysfunctional_dectector/src/pseudofinder_api/merged_pseudo.py -i /mnt/ivy/thliao/project/coral_ruegeria/nanopore_processing/annotations/DFD/s1out/pseudofinder/*/*_nr_pseudos.gff -o /mnt/ivy/thliao/project/coral_ruegeria/nanopore_processing/annotations/DFD/s1out/pseudofinder step2 -gp /mnt/ivy/thliao/project/coral_ruegeria/nanopore_processing/annotations/DFD/s1out/pseudofinder/*/*/gbk

python /home-user/thliao/script/dysfunctional_dectector/src/pseudofinder_api/merged_pseudo.py -i /mnt/ivy/thliao/project/coral_ruegeria/nanopore_processing/tmp/s1out/pseudofinder/F6/F6_nr_pseudos.gff -o /mnt/ivy/thliao/project/coral_ruegeria/nanopore_processing/tmp/s1out/pseudofinder/F6 -snr /mnt/ivy/thliao/project/coral_ruegeria/nanopore_processing/tmp/s1out/pseudofinder/subnr step3 -u /mnt/ivy/thliao/project/coral_ruegeria/nanopore_processing/tmp/s1out/pseudofinder/merged_for/;

python /home-user/thliao/script/dysfunctional_dectector/src/pseudofinder_api/merged_pseudo.py -i /mnt/ivy/thliao/project/coral_ruegeria/nanopore_processing/tmp/s1out/pseudofinder/F6/F6_nr_pseudos.gff -o /mnt/ivy/thliao/project/coral_ruegeria/nanopore_processing/tmp/s1out/pseudofinder/F6 merge 
"""
